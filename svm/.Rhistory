#Model construction
model <- rpart(fmt, data = traindata)# model <- rpart(fmt, data = traindata,control=rpart.control(minsplit=20,cp=0))
#better plotting of the decision tree
library(rpart.plot)
rpart.plot(model, uniform=TRUE, compress=TRUE, branch=.2)
## Test decision tree:
#Test with test data
x_teste <- subset(testdata, select = -Y)
pred <- cbind(predict(model, x_teste ))
predres <-  apply(pred, MARGIN=2, FUN=round0)
cmpdata <- data.frame(actual=testdata$Y, predicted=predres)
nerr <- cnterr(cmpdata$actual, cmpdata$predicted)
errprct <- round(nerr/length(cmpdata$actual)*100, digits=2)
cat(sprintf("Percent errors: %f\n", errprct))
# -- IMPORT DATA --
library("rpart")
library(readxl)
rm(list=ls())
#------------------------------------------------------#
# -- AUXILIARY FUNCS --
round0 <- function(val) {
return(round(val, digits=0))
}
cnterr <- function(V1, V2) {
cnt <- 0
for(i in 1:length(V1)) {
if (V1[i] != V2[i])  {
#cat(sprintf("%d: %s vs %s\n", i, V1[i], V2[i]));
cnt <- cnt+1; }
}
return(cnt)
}
# -- END OF AUXILIARY FUNCS --
#------------------------------------------------------#
#get data from file
df <- read_excel("/Users/MDo/Desktop/AA_Project/project-default-credit-card-clients.xls", skip=1)# skip first collumn
#cleanning the data from file
#replace the spaces by _ in the name of last column, or just use Y, because of future had problems
colnames(df)[25] <- "Y"
names <- names(df)[!names(df) %in% c("ID")]
df <- df[names]
#------------------------------------------------------#
#APPLYING TO DATASET
#------------------------------------------------------#
#create 2 subsets one from training the model and another for testing
#training 2/3, testing 1/3
index <- 1:nrow(df)
testindex <- sample(index, trunc(length(index)/3))
testdata <- na.omit(df[testindex,])
traindata <- na.omit(df[-testindex,])
## Train decision tree:
names <- names(traindata)
fmt <- paste("Y ~", paste(names[!names %in% c("Y", "X")], collapse=" + "))
#Model construction
model <- rpart(fmt, data = traindata)# model <- rpart(fmt, data = traindata,control=rpart.control(minsplit=20,cp=0))
print(model)
#better plotting of the decision tree
library(rpart.plot)
rpart.plot(model, uniform=TRUE, compress=TRUE, branch=.2)
## Test decision tree:
#Test with test data
x_teste <- subset(testdata, select = -Y)
pred <- cbind(predict(model, x_teste ))
predres <-  apply(pred, MARGIN=2, FUN=round0)
cmpdata <- data.frame(actual=testdata$Y, predicted=predres)
nerr <- cnterr(cmpdata$actual, cmpdata$predicted)
errprct <- round(nerr/length(cmpdata$actual)*100, digits=2)
cat(sprintf("Percent errors: %f\n", errprct))
cmpdata
table( cmpdata$actual, cmpdata$predicted )
# -- IMPORT DATA --
library("rpart")
library(readxl)
rm(list=ls())
#------------------------------------------------------#
# -- AUXILIARY FUNCS --
round0 <- function(val) {
return(round(val, digits=0))
}
cnterr <- function(V1, V2) {
cnt <- 0
for(i in 1:length(V1)) {
if (V1[i] != V2[i])  {
#cat(sprintf("%d: %s vs %s\n", i, V1[i], V2[i]));
cnt <- cnt+1; }
}
return(cnt)
}
# -- END OF AUXILIARY FUNCS --
#------------------------------------------------------#
#get data from file
df <- read_excel("/Users/MDo/Desktop/AA_Project/project-default-credit-card-clients.xls", skip=1)# skip first collumn
#cleanning the data from file
#replace the spaces by _ in the name of last column, or just use Y, because of future had problems
colnames(df)[25] <- "Y"
names <- names(df)[!names(df) %in% c("ID")]
df <- df[names]
#------------------------------------------------------#
#APPLYING TO DATASET
#------------------------------------------------------#
#create 2 subsets one from training the model and another for testing
#training 2/3, testing 1/3
index <- 1:nrow(df)
testindex <- sample(index, trunc(length(index)/3))
testdata <- na.omit(df[testindex,])
traindata <- na.omit(df[-testindex,])
## Train decision tree:
names <- names(traindata)
fmt <- paste("Y ~", paste(names[!names %in% c("Y", "X")], collapse=" + "))
#Model construction
model <- rpart(fmt, data = traindata)# model <- rpart(fmt, data = traindata,control=rpart.control(minsplit=20,cp=0))
print(model)
x_train <- subset(traindata, select = -Y)
pred <- predict(model, x_train)
table(pred,x_train)
#better plotting of the decision tree
library(rpart.plot)
rpart.plot(model, uniform=TRUE, compress=TRUE, branch=.2)
table(pred,y)
## Test decision tree:
#Test with test data
x_teste <- subset(testdata, select = -Y)
pred <- cbind(predict(model, x_teste ))
predres <-  apply(pred, MARGIN=2, FUN=round0)
cmpdata <- data.frame(actual=testdata$Y, predicted=predres)
nerr <- cnterr(cmpdata$actual, cmpdata$predicted)
errprct <- round(nerr/length(cmpdata$actual)*100, digits=2)
cat(sprintf("Percent errors: %f\n", errprct))
# -- IMPORT DATA --
library("rpart")
library(readxl)
rm(list=ls())
#------------------------------------------------------#
# -- AUXILIARY FUNCS --
round0 <- function(val) {
return(round(val, digits=0))
}
cnterr <- function(V1, V2) {
cnt <- 0
for(i in 1:length(V1)) {
if (V1[i] != V2[i])  {
#cat(sprintf("%d: %s vs %s\n", i, V1[i], V2[i]));
cnt <- cnt+1; }
}
return(cnt)
}
# -- END OF AUXILIARY FUNCS --
#------------------------------------------------------#
#get data from file
df <- read_excel("/Users/MDo/Desktop/AA_Project/project-default-credit-card-clients.xls", skip=1)# skip first collumn
#cleanning the data from file
#replace the spaces by _ in the name of last column, or just use Y, because of future had problems
colnames(df)[25] <- "Y"
names <- names(df)[!names(df) %in% c("ID")]
df <- df[names]
#------------------------------------------------------#
#APPLYING TO DATASET
#------------------------------------------------------#
#create 2 subsets one from training the model and another for testing
#training 2/3, testing 1/3
index <- 1:nrow(df)
testindex <- sample(index, trunc(length(index)/3))
testdata <- na.omit(df[testindex,])
traindata <- na.omit(df[-testindex,])
## Train decision tree:
names <- names(traindata)
fmt <- paste("Y ~", paste(names[!names %in% c("Y", "X")], collapse=" + "))
#Model construction
model <- rpart(fmt, data = traindata)# model <- rpart(fmt, data = traindata,control=rpart.control(minsplit=20,cp=0))
print(model)
x_train <- subset(traindata, select = -Y)
pred <- predict(model, x_train)
pred
table(pred,x_train)
#better plotting of the decision tree
library(rpart.plot)
rpart.plot(model, uniform=TRUE, compress=TRUE, branch=.2)
table(pred,y)
## Test decision tree:
#Test with test data
x_teste <- subset(testdata, select = -Y)
pred <- cbind(predict(model, x_teste ))
predres <-  apply(pred, MARGIN=2, FUN=round0)
cmpdata <- data.frame(actual=testdata$Y, predicted=predres)
nerr <- cnterr(cmpdata$actual, cmpdata$predicted)
errprct <- round(nerr/length(cmpdata$actual)*100, digits=2)
cat(sprintf("Percent errors: %f\n", errprct))
# -- IMPORT DATA --
library("rpart")
library(readxl)
rm(list=ls())
#------------------------------------------------------#
# -- AUXILIARY FUNCS --
round0 <- function(val) {
return(round(val, digits=0))
}
cnterr <- function(V1, V2) {
cnt <- 0
for(i in 1:length(V1)) {
if (V1[i] != V2[i])  {
#cat(sprintf("%d: %s vs %s\n", i, V1[i], V2[i]));
cnt <- cnt+1; }
}
return(cnt)
}
# -- END OF AUXILIARY FUNCS --
#------------------------------------------------------#
#get data from file
df <- read_excel("/Users/MDo/Desktop/AA_Project/project-default-credit-card-clients.xls", skip=1)# skip first collumn
#cleanning the data from file
#replace the spaces by _ in the name of last column, or just use Y, because of future had problems
colnames(df)[25] <- "Y"
names <- names(df)[!names(df) %in% c("ID")]
df <- df[names]
#------------------------------------------------------#
#APPLYING TO DATASET
#------------------------------------------------------#
#create 2 subsets one from training the model and another for testing
#training 2/3, testing 1/3
index <- 1:nrow(df)
testindex <- sample(index, trunc(length(index)/3))
testdata <- na.omit(df[testindex,])
traindata <- na.omit(df[-testindex,])
## Train decision tree:
names <- names(traindata)
fmt <- paste("Y ~", paste(names[!names %in% c("Y", "X")], collapse=" + "))
#Model construction
model <- rpart(fmt, data = traindata)# model <- rpart(fmt, data = traindata,control=rpart.control(minsplit=20,cp=0))
print(model)
x_train <- subset(traindata, select = -Y)
pred <- predict(model, x_train)
pred
table( apply(pred, MARGIN=2, FUN=round0) , x_train )
#better plotting of the decision tree
library(rpart.plot)
rpart.plot(model, uniform=TRUE, compress=TRUE, branch=.2)
## Test decision tree:
#Test with test data
x_teste <- subset(testdata, select = -Y)
pred <- cbind(predict(model, x_teste ))
predres <-  apply(pred, MARGIN=2, FUN=round0)
cmpdata <- data.frame(actual=testdata$Y, predicted=predres)
table(cmpdata$actual,cmpdata$predicted)
nerr <- cnterr(cmpdata$actual, cmpdata$predicted)
errprct <- round(nerr/length(cmpdata$actual)*100, digits=2)
cat(sprintf("Percent errors: %f\n", errprct))
# -- IMPORT DATA --
library("rpart")
library(readxl)
rm(list=ls())
#------------------------------------------------------#
# -- AUXILIARY FUNCS --
round0 <- function(val) {
return(round(val, digits=0))
}
cnterr <- function(V1, V2) {
cnt <- 0
for(i in 1:length(V1)) {
if (V1[i] != V2[i])  {
#cat(sprintf("%d: %s vs %s\n", i, V1[i], V2[i]));
cnt <- cnt+1; }
}
return(cnt)
}
# -- END OF AUXILIARY FUNCS --
#------------------------------------------------------#
#get data from file
df <- read_excel("/Users/MDo/Desktop/AA_Project/project-default-credit-card-clients.xls", skip=1)# skip first collumn
#cleanning the data from file
#replace the spaces by _ in the name of last column, or just use Y, because of future had problems
colnames(df)[25] <- "Y"
names <- names(df)[!names(df) %in% c("ID")]
df <- df[names]
#------------------------------------------------------#
#APPLYING TO DATASET
#------------------------------------------------------#
#create 2 subsets one from training the model and another for testing
#training 2/3, testing 1/3
index <- 1:nrow(df)
testindex <- sample(index, trunc(length(index)/3))
testdata <- na.omit(df[testindex,])
traindata <- na.omit(df[-testindex,])
## Train decision tree:
names <- names(traindata)
fmt <- paste("Y ~", paste(names[!names %in% c("Y", "X")], collapse=" + "))
#Model construction
model <- rpart(fmt, data = traindata)# model <- rpart(fmt, data = traindata,control=rpart.control(minsplit=20,cp=0))
print(model)
#better plotting of the decision tree
library(rpart.plot)
rpart.plot(model, uniform=TRUE, compress=TRUE, branch=.2)
## Test decision tree:
#Test with test data
x_teste <- subset(testdata, select = -Y)
pred <- cbind(predict(model, x_teste ))
predres <-  apply(pred, MARGIN=2, FUN=round0)
cmpdata <- data.frame(actual=testdata$Y, predicted=predres)
#confusion table
table(cmpdata$actual,cmpdata$predicted)
nerr <- cnterr(cmpdata$actual, cmpdata$predicted)
errprct <- round(nerr/length(cmpdata$actual)*100, digits=2)
cat(sprintf("Percent errors: %f\n", errprct))
# -- IMPORT DATA --
#neuralnet library
library("neuralnet")
# read data from xls
library(readxl)
rm(list=ls())
#------------------------------------------------------#
# -- AUXILIARY FUNCS --
#func to return a random # of rows from dataset
#this will be used to tune the model Vars on a smaller set
randomRows = function(df,n){
return(df[sample(nrow(df),n),])
}
#normalization function
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
round0 <- function(val) {
return(round(val, digits=0))
}
cnterr <- function(V) {
cnt <- 0
for(i in 1:length(V)) {
if (V[i] != 0) { cnt <- cnt+1; }
}
return(cnt)
}
# -- END OF AUXILIARY FUNCS --
#------------------------------------------------------#
#get data from file
df <- read_excel("/Users/MDo/Desktop/AA_Project/project-default-credit-card-clients.xls", skip=1)# skip first collumn
# check if data is missing
apply(df, 2, function (x) sum(is.na(x))) # no NA -> good
#cleanning the data from file
#replace the spaces by _ in the name of last column, or just use Y, because of future had problems
colnames(df)[25] <- "Y"
names <- names(df)[!names(df) %in% c("ID")]
clean_data <- df[names]
# normalize input with min-max
maxs <- apply(clean_data, 2, max)
mins <- apply(clean_data, 2, min)
normalized_data <- as.data.frame(scale(clean_data, center = mins, scale = maxs - mins))
#------------------------------------------------------#
#APPLYING TO THE WHOLE DATASET
#------------------------------------------------------#
#create 2 subsets one from training the model and another for testing
#training 2/3, testing 1/3
index <- 1:nrow(normalized_data)
testindex <- sample(index, trunc(length(index)/3))
testdata <- na.omit(normalized_data[testindex,])
traindata <- na.omit(normalized_data[-testindex,])
nn <- neuralnet(paste("Y ~", paste(names[!names %in% c("Y")],collapse=" + ")),
traindata,
#i will go with the h6 set of hidden layers
#its the one with results fitting the other sets and with a lower time
hidden=c(12),
act.fct="logistic",
linear.output=FALSE,
threshold=0.1,
lifesign = "minimal")
#print(nn)
plot(nn)
## Test MLP
cmpv<-data.frame(actual=traindata$Y, predicted=nn$net.result)
names(cmpv) <- sub("^structure.*", "predicted", names(cmpv))
print(cmpv[1100:1150, ])
tstdata <- subset(testdata, select = !names(testdata) %in% c("Y"))
nn.pred <- compute(nn, tstdata)$net.result
#rounds the nn.pred values to 1 or 0 by a margin of 2 decimal points
predres <- apply(nn.pred, MARGIN=2, FUN=round0)
cmpdata <- data.frame(actual=testdata$Y, predicted=predres)
print(cmpdata[1100:1150, ])
table( actual=testdata$Y, predicted=predres )
#TEST______________________________
pr.nn_ <- nn.pred*(max(df$Y)-min(df$Y))+min(df$Y)
test.r <- (testdata$Y)*(max(df$Y)-min(df$Y))+min(df$Y)
#medium square error
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(testdata)
plot( testdata[[1]] , pr.nn_ ,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
#TEST______________________________
m<-glm(Y~.,data=traindata,family=binomial())
test<-predict(m,type='response',testdata)
pred__<-prediction(test,testdata$Y)
perf <- performance(pred__,"tpr","fpr")
plot(perf)
#TEST______________________________
#ROC for my data
pred5<-prediction(nn.pred,testdata$Y)
pred6<-prediction(predres,testdata$Y)
perf <- performance(pred6,"tpr","fpr")
plot(perf)
#TEST______________________________
nerr <- cnterr(cmpdata$actual-cmpdata$predicted)
errprct <- round(nerr/length(cmpdata$actual)*100, digits=2)
cat(sprintf("Percent errors: %f\n", errprct))
# -- IMPORT DATA --
#neuralnet library
library("neuralnet")
# read data from xls
library(readxl)
rm(list=ls())
#------------------------------------------------------#
# -- AUXILIARY FUNCS --
#func to return a random # of rows from dataset
#this will be used to tune the model Vars on a smaller set
randomRows = function(df,n){
return(df[sample(nrow(df),n),])
}
#normalization function
normalize <- function(x) {
return((x - min(x)) / (max(x) - min(x)))
}
round0 <- function(val) {
return(round(val, digits=0))
}
cnterr <- function(V) {
cnt <- 0
for(i in 1:length(V)) {
if (V[i] != 0) { cnt <- cnt+1; }
}
return(cnt)
}
# -- END OF AUXILIARY FUNCS --
#------------------------------------------------------#
#get data from file
df <- read_excel("/Users/MDo/Desktop/AA_Project/project-default-credit-card-clients.xls", skip=1)# skip first collumn
# check if data is missing
apply(df, 2, function (x) sum(is.na(x))) # no NA -> good
#cleanning the data from file
#replace the spaces by _ in the name of last column, or just use Y, because of future had problems
colnames(df)[25] <- "Y"
names <- names(df)[!names(df) %in% c("ID")]
clean_data <- df[names]
# normalize input with min-max
maxs <- apply(clean_data, 2, max)
mins <- apply(clean_data, 2, min)
normalized_data <- as.data.frame(scale(clean_data, center = mins, scale = maxs - mins))
#------------------------------------------------------#
#APPLYING TO THE WHOLE DATASET
#------------------------------------------------------#
#create 2 subsets one from training the model and another for testing
#training 2/3, testing 1/3
index <- 1:nrow(normalized_data)
testindex <- sample(index, trunc(length(index)/3))
testdata <- na.omit(normalized_data[testindex,])
traindata <- na.omit(normalized_data[-testindex,])
nn <- neuralnet(paste("Y ~", paste(names[!names %in% c("Y")],collapse=" + ")),
traindata,
#i will go with the h6 set of hidden layers
#its the one with results fitting the other sets and with a lower time
hidden=c(12),
act.fct="logistic",
linear.output=FALSE,
threshold=0.1,
lifesign = "minimal")
#print(nn)
plot(nn)
## Test MLP
cmpv<-data.frame(actual=traindata$Y, predicted=nn$net.result)
names(cmpv) <- sub("^structure.*", "predicted", names(cmpv))
print(cmpv[1100:1150, ])
tstdata <- subset(testdata, select = !names(testdata) %in% c("Y"))
nn.pred <- compute(nn, tstdata)$net.result
#rounds the nn.pred values to 1 or 0 by a margin of 2 decimal points
predres <- apply(nn.pred, MARGIN=2, FUN=round0)
cmpdata <- data.frame(actual=testdata$Y, predicted=predres)
print(cmpdata[1100:1150, ])
table( actual=testdata$Y, predicted=predres )
# #TEST______________________________
# pr.nn_ <- nn.pred*(max(df$Y)-min(df$Y))+min(df$Y)
# test.r <- (testdata$Y)*(max(df$Y)-min(df$Y))+min(df$Y)
# #medium square error
# MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(testdata)
#
# plot( testdata[[1]] , pr.nn_ ,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
# abline(0,1,lwd=2)
# legend('bottomright',legend='NN',pch=18,col='red', bty='n')
# #TEST______________________________
# m<-glm(Y~.,data=traindata,family=binomial())
# test<-predict(m,type='response',testdata)
# pred__<-prediction(test,testdata$Y)
# perf <- performance(pred__,"tpr","fpr")
# plot(perf)
# #TEST______________________________
# #ROC for my data
# pred5<-prediction(nn.pred,testdata$Y)
# pred6<-prediction(predres,testdata$Y)
# perf <- performance(pred6,"tpr","fpr")
# plot(perf)
# #TEST______________________________
nerr <- cnterr(cmpdata$actual-cmpdata$predicted)
errprct <- round(nerr/length(cmpdata$actual)*100, digits=2)
cat(sprintf("Percent errors: %f\n", errprct))
library(ROCR)
library(ROCR)
ROCRpred1=prediction(nn.pred)
ROCRpred1=prediction( nn.pred, tstdata )
ROCRpred1=prediction( actual=traindata$Y, predicted=nn$net.result )
ROCRpred1=prediction( nn$net.result, testdata$Y )
ROCRpred1=prediction(predres,testdata$Y)
ROCRperf1=performance(ROCRpred1,"tpr","fpr")
plot(ROCRperf1)
ROCRpred1=prediction(nn.pred,testdata$Y)
ROCRperf1=performance(ROCRpred1,"tpr","fpr")
plot(ROCRperf1)
abline(0,1,lwd=2)
source('~/Desktop/AA_Project/nn/neuralnet.R')
plot(ROCR_perf)
